{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At the moment this notebook contains a copy of the code in Sequential_Menu_Search.ipynb. Your task is to edit this code in such a way that the agent can jump to any position in the menu--that is, it is not restricted to move one item at a time. \n",
    "\n",
    "Hint: Number of possible Actions are now equivalent to the number of items in the menu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from random import random, choice\n",
    "from scipy import zeros\n",
    "import numpy as np\n",
    "\n",
    "from pybrain.utilities import Named\n",
    "from pybrain.rl.environments.environment import Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Menu(Environment, Named):\n",
    "    \"\"\" 2D mazes, with actions being the direction of movement (N,E,S,W)\n",
    "    Note that in this example it is a menu represeted as a one dimesional array.\n",
    "    Hence, the user can traverse only in left (west) or right (east) directions\n",
    "    The observations can be noisy.\n",
    "    \"\"\"\n",
    "\n",
    "    # table of booleans\n",
    "    mazeTable = None\n",
    "\n",
    "    # single goal\n",
    "    goal = None\n",
    "\n",
    "    # current state\n",
    "    perseus = None\n",
    "\n",
    "    # list of possible initial states\n",
    "    initPos = None\n",
    "\n",
    "    # directions\n",
    "    N = (1, 0)\n",
    "    S = (-1, 0)\n",
    "    E = (0, 1)\n",
    "    W = (0, -1)\n",
    "\n",
    "    #note that the action space contains only east or west movements\n",
    "    allActions = [E,W]\n",
    "\n",
    "    # stochasticity\n",
    "    stochAction = 0.\n",
    "    stochObs = 0.\n",
    "\n",
    "    def __init__(self, topology, goal, **args):\n",
    "        allActions = range(0,np.size(topology),1)\n",
    "        print allActions\n",
    "\n",
    "        self.setArgs(**args)\n",
    "        self.mazeTable = topology\n",
    "        self.goal = goal\n",
    "        if self.initPos == None:\n",
    "            self.initPos = self._freePos()\n",
    "            self.initPos.remove(self.goal)\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\" return to initial position (stochastically): \"\"\"\n",
    "        self.bang = False\n",
    "        self.perseus = choice(self.initPos)\n",
    "\n",
    "    def _freePos(self):\n",
    "        \"\"\" produce a list of the free positions. \"\"\"\n",
    "        res = []\n",
    "        for i, row in enumerate(self.mazeTable):\n",
    "            for j, p in enumerate(row):\n",
    "                if p == False:\n",
    "                    res.append((i, j))\n",
    "        return res\n",
    "\n",
    "    def _moveInDir(self, pos, dir):\n",
    "        \"\"\" the new state after the movement in one direction. \"\"\"\n",
    "        return (pos[0] + dir[0], pos[1] + dir[1])\n",
    "\n",
    "    def performAction(self, action):\n",
    "\n",
    "        if self.stochAction > 0:\n",
    "            if random() < self.stochAction:\n",
    "                action = choice(list(range(len(self.allActions))))\n",
    "\n",
    "        tmp = self._moveInDir(self.perseus, self.allActions[action])\n",
    "        \n",
    "        if self.mazeTable[tmp] == False:\n",
    "            self.perseus = tmp\n",
    "            self.bang = False\n",
    "        else:\n",
    "            self.bang = True\n",
    "\n",
    "    def getSensors(self):\n",
    "        obs = zeros(2)\n",
    "        for i, a in enumerate(Maze.allActions):\n",
    "            obs[i] = self.mazeTable[self._moveInDir(self.perseus, a)]\n",
    "        if self.stochObs > 0:\n",
    "            for i in range(len(obs)):\n",
    "                if random() < self.stochObs:\n",
    "                    obs[i] = not obs[i]\n",
    "        return obs\n",
    "\n",
    "    def __str__(self):\n",
    "        \"\"\" Ascii representation of the maze, with the current state \"\"\"\n",
    "        s = ''\n",
    "        for r, row in reversed(list(enumerate(self.mazeTable))):\n",
    "            for c, p in enumerate(row):\n",
    "                if (r, c) == self.goal:\n",
    "                    s += '*'\n",
    "                elif (r, c) == self.perseus:\n",
    "                    s += '@'\n",
    "                elif p == True:\n",
    "                    s += '#'\n",
    "                else:\n",
    "                    s += ' '\n",
    "            s += '\\n'\n",
    "        return s\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pybrain.rl.environments import Task\n",
    "from scipy import array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class MDPMenuTask(Task):\n",
    "    \"\"\" This is a MDP task for the MenuEnvironment. The state is fully observable,\n",
    "        giving the agent the current position of perseus. Reward is given on reaching\n",
    "        the goal, otherwise no reward. \"\"\"\n",
    "\n",
    "    def getReward(self):\n",
    "        \"\"\" compute and return the current reward (i.e. corresponding to the last action performed)\n",
    "        Note that now we give a negative reward for visiting irrelevant menu items.\"\"\"\n",
    "        if self.env.goal == self.env.perseus:\n",
    "            self.env.reset()\n",
    "            reward = 1.\n",
    "        else:\n",
    "            reward = -1.\n",
    "        return reward\n",
    "\n",
    "    def performAction(self, action):\n",
    "        \"\"\" The action vector is stripped and the only element is cast to integer and given\n",
    "            to the super class.\n",
    "        \"\"\"\n",
    "        Task.performAction(self, int(action[0]))\n",
    "\n",
    "\n",
    "    def getObservation(self):\n",
    "        \"\"\" The agent receives its position in the menu, to make this a fully observable\n",
    "            MDP problem.\n",
    "        \"\"\"\n",
    "        obs = array([self.env.perseus[0] * self.env.mazeTable.shape[0] + self.env.perseus[1]])\n",
    "        return obs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from scipy import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pybrain.rl.learners.valuebased import ActionValueTable\n",
    "from pybrain.rl.agents import LearningAgent\n",
    "from pybrain.rl.learners import Q\n",
    "from pybrain.rl.experiments import Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pylab\n",
    "pylab.gray()\n",
    "pylab.ion()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create the menu with 7 items\n",
    "matrix_size = 7\n",
    "envmatrix = array([[0,0,0,0,0,0,1]])\n",
    "\n",
    "env = Menu(envmatrix,(0,2))\n",
    "\n",
    "# create task\n",
    "task = MDPMenuTask(env)\n",
    "\n",
    "\n",
    "\n",
    "# create value table and initialize with ones\n",
    "table = ActionValueTable(matrix_size, 2)\n",
    "table.initialize(1.)\n",
    "\n",
    "\n",
    "# create agent with controller and learner - use SARSA(), Q() or QLambda() here\n",
    "learner = Q()\n",
    "\n",
    "# standard exploration is e-greedy, but a different type can be chosen as well, such as:\n",
    "# learner.explorer = BoltzmannExplorer()\n",
    "\n",
    "# create agent\n",
    "agent = LearningAgent(table, learner)\n",
    "\n",
    "# create experiment\n",
    "experiment = Experiment(task, agent)\n",
    "\n",
    "# prepare plotting\n",
    "pylab.gray()\n",
    "pylab.ion()\n",
    "\n",
    "for i in range(100):\n",
    "    # interact with the environment (here in batch mode)\n",
    "    experiment.doInteractions(matrix_size)\n",
    "    agent.learn()\n",
    "    agent.reset()\n",
    "    # and draw the table\n",
    "    pylab.pcolor(table.params.reshape(matrix_size,2).max(1).reshape(matrix_size,1))\n",
    "    pylab.draw()\n",
    "    pylab.ion()\n",
    "    pylab.show()\n",
    "print \"training complete\"\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
