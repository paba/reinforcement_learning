{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to edit the Maze example to work on the menu search scenario. Here we have three classes:\n",
    "1. Menu class, which implements the Environment class in PyBrain\n",
    "2. MDPMenuTask class, which implements the Task class in PyBrain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First import the libraries you need for the Menu environment definition. Note that we import the class Environment from pybrain.rl.environments.environment. The Menu class inherits from Environment and implements the abstract initialization method, reset(), performAction() and getSensors(). It is a good idea to take a closer look at these methods, because they define how our environment behaves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from random import random, choice\n",
    "from scipy import zeros\n",
    "import numpy as np\n",
    "\n",
    "from pybrain.utilities import Named\n",
    "from pybrain.rl.environments.environment import Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Menu(Environment, Named):\n",
    "    \"\"\" 2D mazes, with actions being the direction of movement (N,E,S,W)\n",
    "    Note that in this example it is a menu represeted as a one dimesional array.\n",
    "    Hence, the user can traverse only in left (west) or right (east) directions\n",
    "    The observations can be noisy.\n",
    "    \"\"\"\n",
    "\n",
    "    # table of booleans\n",
    "    mazeTable = None\n",
    "\n",
    "    # single goal\n",
    "    goal = None\n",
    "\n",
    "    # current state\n",
    "    perseus = None\n",
    "\n",
    "    # list of possible initial states\n",
    "    initPos = None\n",
    "\n",
    "    # directions the agent can move. \n",
    "    N = (1, 0)\n",
    "    S = (-1, 0)\n",
    "    E = (0, 1)\n",
    "    W = (0, -1)\n",
    "\n",
    "    #note that the action space contains only east or west movements\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below (allActions = [E,W]) is one of the first codes we need to change from the maze example. In a Maze you can move in all 4 directions (N,S,E,W). However, in the menu your movement is restricted to two directions (up and down/ left and right). If we define the menu as a 1-dimensional array then the possible actions as E (moving to right) and W (moving to left). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "    allActions = [E,W]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We set stochasticity to 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "    # stochasticity\n",
    "    stochAction = 0.\n",
    "    stochObs = 0.\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The __init__ method initiales parameters of the environemnt before training. Topology is the structure of the menu, and the goal defines the location of the target item. We assume that all the items in this menu are irrelevant (relevance = 0) except for the target item, where relevance = 1. This is the simplest menu we could think of, a more complex menu would be a semantic menu, where there are items partially relevant to the target. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "    def __init__(self, topology, goal, **args):\n",
    "        allActions = range(0,np.size(topology),1)\n",
    "        print allActions\n",
    "\n",
    "        self.setArgs(**args)\n",
    "        self.mazeTable = topology\n",
    "        self.goal = goal\n",
    "        if self.initPos == None:\n",
    "            self.initPos = self._freePos()\n",
    "            self.initPos.remove(self.goal)\n",
    "        self.reset()\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The reset function is called whenever a new training trial is started. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "    def reset(self):\n",
    "        \"\"\" return to initial position (stochastically): \"\"\"\n",
    "        self.bang = False\n",
    "        self.perseus = choice(self.initPos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_freePos and _moveInDir functions are specific to this example. _moveInDir finds the gaze location when the action coordinates --dir-- and  the current position coordinates --pos -- are given. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "    def _freePos(self):\n",
    "        \"\"\" produce a list of the free positions. \"\"\"\n",
    "        res = []\n",
    "        for i, row in enumerate(self.mazeTable):\n",
    "            for j, p in enumerate(row):\n",
    "                if p == False:\n",
    "                    res.append((i, j))\n",
    "        return res\n",
    "\n",
    "    def _moveInDir(self, pos, dir):\n",
    "        \"\"\" the new state after the movement in one direction. \"\"\"\n",
    "        return (pos[0] + dir[0], pos[1] + dir[1])\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "performAction is an important function. This is called whenever the agent choses an action. First we compute the current location --tmp-- by using _moveInDir function. Then we check whether the new position is a wall or not. If it is a wall, the agent bangs on the wall (bang = True). Otherwise we set the coordinates of the new position (perseus) to the new location and set bang to False. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "    def performAction(self, action):\n",
    "\n",
    "        if self.stochAction > 0:\n",
    "            if random() < self.stochAction:\n",
    "                action = choice(list(range(len(self.allActions))))\n",
    "\n",
    "        tmp = self._moveInDir(self.perseus, self.allActions[action])\n",
    "        \n",
    "        if self.mazeTable[tmp] == False:\n",
    "            self.perseus = tmp\n",
    "            self.bang = False\n",
    "        else:\n",
    "            self.bang = True\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the getSensort function returns the state_id -- which is a unique id for each specific state the agent can be in. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "    def getSensors(self):\n",
    "        obs = zeros(2)\n",
    "        for i, a in enumerate(Maze.allActions):\n",
    "            obs[i] = self.mazeTable[self._moveInDir(self.perseus, a)]\n",
    "        if self.stochObs > 0:\n",
    "            for i in range(len(obs)):\n",
    "                if random() < self.stochObs:\n",
    "                    obs[i] = not obs[i]\n",
    "        return obs\n",
    "\n",
    "    def __str__(self):\n",
    "        \"\"\" Ascii representation of the maze, with the current state \"\"\"\n",
    "        s = ''\n",
    "        for r, row in reversed(list(enumerate(self.mazeTable))):\n",
    "            for c, p in enumerate(row):\n",
    "                if (r, c) == self.goal:\n",
    "                    s += '*'\n",
    "                elif (r, c) == self.perseus:\n",
    "                    s += '@'\n",
    "                elif p == True:\n",
    "                    s += '#'\n",
    "                else:\n",
    "                    s += ' '\n",
    "            s += '\\n'\n",
    "        return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pybrain.rl.environments import Task\n",
    "from scipy import array"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Now wer are going to implement the Task. Pay special attention to the getReward function. We give a negative reward (-1) for all the actions that do not lead to the goal and a positive reward +1 when you have reached the goal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class MDPMenuTask(Task):\n",
    "    \"\"\" This is a MDP task for the MenuEnvironment. The state is fully observable,\n",
    "        giving the agent the current position of perseus. Reward is given on reaching\n",
    "        the goal, otherwise no reward. \"\"\"\n",
    "\n",
    "    def getReward(self):\n",
    "        \"\"\" compute and return the current reward (i.e. corresponding to the last action performed)\n",
    "        Note that now we give a negative reward for visiting irrelevant menu items.\"\"\"\n",
    "        if self.env.goal == self.env.perseus:\n",
    "            self.env.reset()\n",
    "            reward = 1.\n",
    "        else:\n",
    "            reward = -1.\n",
    "        return reward\n",
    "\n",
    "    def performAction(self, action):\n",
    "        \"\"\" The action vector is stripped and the only element is cast to integer and given\n",
    "            to the super class.\n",
    "        \"\"\"\n",
    "        Task.performAction(self, int(action[0]))\n",
    "\n",
    "\n",
    "    def getObservation(self):\n",
    "        \"\"\" The agent receives its position in the menu, to make this a fully observable\n",
    "            MDP problem.\n",
    "        \"\"\"\n",
    "        obs = array([self.env.perseus[0] * self.env.mazeTable.shape[0] + self.env.perseus[1]])\n",
    "        return obs\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from scipy import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "from pybrain.rl.learners.valuebased import ActionValueTable\n",
    "from pybrain.rl.agents import LearningAgent\n",
    "from pybrain.rl.learners import Q\n",
    "from pybrain.rl.experiments import Experiment\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pylab\n",
    "pylab.gray()\n",
    "pylab.ion()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# create the menu with 7 items\n",
    "matrix_size = 7\n",
    "envmatrix = array([[0,0,0,0,0,0,1]])\n",
    "\n",
    "env = Menu(envmatrix,(0,2))\n",
    "\n",
    "# create task\n",
    "task = MDPMenuTask(env)\n",
    "\n",
    "\n",
    "\n",
    "# create value table and initialize with ones\n",
    "table = ActionValueTable(matrix_size, 2)\n",
    "table.initialize(1.)\n",
    "\n",
    "\n",
    "# create agent with controller and learner - use SARSA(), Q() or QLambda() here\n",
    "learner = Q()\n",
    "\n",
    "# standard exploration is e-greedy, but a different type can be chosen as well, such as:\n",
    "# learner.explorer = BoltzmannExplorer()\n",
    "\n",
    "# create agent\n",
    "agent = LearningAgent(table, learner)\n",
    "\n",
    "# create experiment\n",
    "experiment = Experiment(task, agent)\n",
    "\n",
    "# prepare plotting\n",
    "pylab.gray()\n",
    "pylab.ion()\n",
    "\n",
    "for i in range(100):\n",
    "    # interact with the environment (here in batch mode)\n",
    "    experiment.doInteractions(matrix_size)\n",
    "    agent.learn()\n",
    "    agent.reset()\n",
    "    # and draw the table\n",
    "    pylab.pcolor(table.params.reshape(matrix_size,2).max(1).reshape(matrix_size,1))\n",
    "    pylab.draw()\n",
    "    pylab.ion()\n",
    "    pylab.show()\n",
    "print \"training complete\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
