{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This tutorial walks you through a reinforcement learning example implemented with pybrain. \n",
    "In this document we walk you through all the td.py code. If you want to see the output without going through these steps, run td.py (python td.py). \n",
    "\n",
    "The first step is to import some general packages and the RL components from PyBrain.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\" This example demonstrates how to use the discrete Temporal Difference\n",
    "Reinforcement Learning algorithms (SARSA, Q, Q(lambda)) in a classical\n",
    "fully observable MDP maze task. The goal point is the top right free\n",
    "field. \"\"\"\n",
    "from scipy import *\n",
    "import sys, time\n",
    "import pylab\n",
    "\n",
    "#from pybrain.rl.environments.mazes import Maze, MDPMazeTask\n",
    "from maze import Maze\n",
    "from mdp import MDPMazeTask\n",
    "from pybrain.rl.learners.valuebased import ActionValueTable\n",
    "from pybrain.rl.agents import LearningAgent\n",
    "from pybrain.rl.learners import Q, QLambda, SARSA #@UnusedImport\n",
    "from pybrain.rl.explorers import BoltzmannExplorer #@UnusedImport\n",
    "from pybrain.rl.experiments import Experiment\n",
    "from pybrain.rl.environments import Task"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Now we specify the environment in which the agent interacts. Take a look at the file maze.py. It implements the Environment class in pybrain. In the next tutorial we will go through the maze.py implementation in more detail. For now, note that the performAction() method specifies what to do according to the action chosen by the agent. \n",
    "\n",
    "In this example we specify the environment as a maze.  It creates a labyrinth with free fields, walls, and an goal point. An agent can move over the free fields and needs to find the goal point. Letâ€™s define the maze structure, a simple 2D numpy array, where 1 is a wall and 0 is a free field:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create the maze with walls (1)\n",
    "envmatrix = array([[1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
    "                   [1, 0, 0, 1, 0, 0, 0, 0, 1],\n",
    "                   [1, 0, 0, 1, 0, 0, 1, 0, 1],\n",
    "                   [1, 0, 0, 1, 0, 0, 1, 0, 1],\n",
    "                   [1, 0, 0, 1, 0, 1, 1, 0, 1],\n",
    "                   [1, 0, 0, 0, 0, 0, 1, 0, 1],\n",
    "                   [1, 1, 1, 1, 1, 1, 1, 0, 1],\n",
    "                   [1, 0, 0, 0, 0, 0, 0, 0, 1],\n",
    "                   [1, 1, 1, 1, 1, 1, 1, 1, 1]])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Then we create the environment with the structure as first parameter and the goal field tuple as second parameter:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#goal is set to cell 7,7\n",
    "env = Maze(envmatrix, (7, 7))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we specify the agent. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "table = ActionValueTable(81, 4)\n",
    "table.initialize(1.)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "create agent with controller and learner - use SARSA(), Q() or QLambda() here. We are using Q-learning. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "learner = Q()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "standard exploration is e-greedy, but a different type can be chosen as well. For example, learner.explorer = BoltzmannExplorer(). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "agent = LearningAgent(table, learner)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "create the task and the experiment. Task is implemented in the mdp.py file. Open the mdp.py and try to understand what is included in the task. In the mdp.py file, there is a class MDPMazeTask, which implements the Task class in pybrain. Note that the reward function is defined in the Task class (see getReward() function). Take a good look at the reward function. What is the reward the agent getting? (Answer: it gets a positive reward, which is equal to 1, only when it reaches the goal, in all the other states it receives no reward (reward = 0)) \n",
    "If inerested play with this reward function, see how it affects the behavior of the agent. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "task = MDPMazeTask(env)\n",
    "experiment = Experiment(task, agent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the training begins. In the for loop we specify the number of training episodes. Here we have set it to 1000. In each iteration the agent trieds 100 actions (that means 100 moves within the maze). Each move results in learning and updating the Q-table. After one training episode (100 moves), we call the agent.reset() function. This function is implemented in the environment in maze.py. Note that the reset() function does not reset the knowledge gained so far through learning (Q-table), instead it initializes the environment (rewards, maze, and other parameters used in the computation) to begin a new training episode.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training complete\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "for i in range(1000):\n",
    "    experiment.doInteractions(100)\n",
    "    agent.learn()\n",
    "    agent.reset()\n",
    "\n",
    "    pylab.pcolor(table.params.reshape(81,4).max(1).reshape(9,9))\n",
    "    pylab.draw()\n",
    "    pylab.ion()\n",
    "    pylab.show()\n",
    "    \n",
    "print \"training complete\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
